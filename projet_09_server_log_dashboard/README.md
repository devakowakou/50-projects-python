# ğŸ“Š Analyseur de Logs Web AvancÃ©

Plateforme complÃ¨te d'analytics pour transformer vos logs serveur en insights stratÃ©giques avec Machine Learning et visualisations interactives.

## âœ¨ FonctionnalitÃ©s

- ğŸ“ˆ **Analyse en temps rÃ©el** : Traitement de **millions de lignes** de logs
- ğŸ‘¥ **DÃ©tection de sessions** : Parcours clients et taux de rebond automatiques
- ğŸ¤– **Machine Learning** : DÃ©tection d'anomalies avec Isolation Forest (Scikit-learn)
- ğŸŒ **Benchmarking** : Web scraping avec BeautifulSoup pour comparer sites concurrents
- ğŸ’¡ **Recommandations** : Insights et actions stratÃ©giques automatiques
- ğŸ“Š **Visualisations** : Graphiques interactifs avec Plotly + Matplotlib/Seaborn
- âš¡ **Performance** : Traitement parallÃ¨le multicore pour gros volumes

## ğŸš€ Installation et gÃ©nÃ©ration de donnÃ©es

### 1. Installation

```bash
cd projet_09_server_log_dashboard
python -m venv venv
source venv/bin/activate

pip install -r backend/requirements.txt
pip install -r frontend/requirements.txt
```

### 2. GÃ©nÃ©rer des donnÃ©es MASSIVES (recommandÃ©)

**Option A : 2 millions de logs (production-like)**
```bash
python scripts/generate_massive_logs.py
# âš¡ GÃ©nÃ©ration parallÃ¨le multicore
# âœ… ~200MB de logs en <1 minute
```

**Option B : Logs de dÃ©mo (5K lignes)**
```bash
python scripts/generate_sample_logs.py
```

**Option C : Streaming temps rÃ©el**
```bash
python scripts/stream_logs_realtime.py
# ğŸŒŠ 10 logs/seconde en continu
```

### 3. Importer en base de donnÃ©es

**Import parallÃ¨le haute performance :**
```bash
python scripts/import_production_logs.py
# ğŸš€ Utilise tous les cores CPU
# âš¡ ~50K logs/seconde
```

### 4. Lancer l'application

**Terminal 1 - Backend API:**
```bash
cd backend
uvicorn main:app --reload
```

**Terminal 2 - Frontend Streamlit:**
```bash
cd frontend
streamlit run app.py
```

AccÃ©dez Ã  :
- Frontend: http://localhost:8501
- API Docs: http://localhost:8000/docs

## ğŸ“Š Performances mesurÃ©es

- âš¡ **GÃ©nÃ©ration**: 100K logs/seconde (parallÃ¨le)
- ğŸ’¾ **Import DB**: 50K logs/seconde (multicore)
- ğŸ§  **ML Training**: 2M logs analysÃ©s en <5 secondes
- ğŸš€ **API Response**: <50ms pour requÃªtes complexes
- ğŸ“ˆ **ScalabilitÃ©**: TestÃ© avec 10M+ logs

## ğŸ› ï¸ Stack technique

- **Backend**: FastAPI, SQLAlchemy, SQLite
- **ML**: Scikit-learn (Isolation Forest, StandardScaler)
- **Data Processing**: Pandas, NumPy (vectorisation)
- **Frontend**: Streamlit, Plotly, Matplotlib, Seaborn
- **Scraping**: BeautifulSoup4, Requests
- **ParallÃ©lisation**: ProcessPoolExecutor, multiprocessing

## ğŸ“ Structure du projet

```
projet_09_server_log_dashboard/
â”‚
â”œâ”€â”€ backend/                  # Code source de l'API FastAPI
â”‚   â”œâ”€â”€ app/                  # Modules de l'application
â”‚   â”œâ”€â”€ tests/                # Tests unitaires
â”‚   â”œâ”€â”€ Dockerfile            # Image Docker pour l'API
â”‚   â””â”€â”€ requirements.txt      # DÃ©pendances Python
â”‚
â”œâ”€â”€ frontend/                 # Code source du dashboard Streamlit
â”‚   â”œâ”€â”€ pages/                # Pages du dashboard
â”‚   â”œâ”€â”€ components/           # Composants rÃ©utilisables
â”‚   â”œâ”€â”€ Dockerfile            # Image Docker pour le frontend
â”‚   â””â”€â”€ requirements.txt      # DÃ©pendances Python
â”‚
â”œâ”€â”€ docs/                    # Documentation du projet
â”‚   â”œâ”€â”€ screenshots/          # Captures d'Ã©cran
â”‚   â””â”€â”€ rapport.md            # Rapport d'analyse
â”‚
â”œâ”€â”€ scripts/                 # Scripts utilitaires
â”‚   â”œâ”€â”€ generate_sample_logs.py # GÃ©nÃ©ration de logs de test
â”‚   â””â”€â”€ import_logs_to_db.py  # Importation des logs dans la DB
â”‚
â”œâ”€â”€ docker/                  # Fichiers Docker
â”‚   â”œâ”€â”€ docker-compose.yml    # Configuration Docker Compose
â”‚   â””â”€â”€ nginx.conf           # Configuration Nginx
â”‚
â”œâ”€â”€ .env                     # Variables d'environnement
â”œâ”€â”€ README.md                # Documentation principale
â””â”€â”€ requirements.txt         # DÃ©pendances communes
```

## ğŸ“¸ Screenshots

### Dashboard Principal
![Dashboard](docs/screenshots/dashboard.png)

### Analyse de Sessions
![Sessions](docs/screenshots/sessions.png)

### DÃ©tection d'Anomalies ML
![Anomalies](docs/screenshots/anomalies.png)

## ğŸ¯ Cas d'usage

1. **Monitoring production** : Surveillance en temps rÃ©el de vos serveurs
2. **Analyse post-incident** : Investigation aprÃ¨s une panne
3. **Optimisation SEO** : Identification des pages Ã  problÃ¨mes
4. **Benchmarking concurrent** : Comparer vos performances
5. **Reporting client** : GÃ©nÃ©ration de rapports automatiques

## ğŸ” SÃ©curitÃ©

- âœ… Pas de credentials en dur (utilise .env)
- âœ… CORS configurÃ© pour production
- âœ… Validation des inputs avec Pydantic
- âœ… Rate limiting recommandÃ© (ajout manuel)
- âœ… HTTPS obligatoire en production

## ğŸš€ DÃ©ploiement production

### Option 1: Docker Compose
```bash
docker-compose -f docker/docker-compose.yml up -d
```

### Option 2: Services sÃ©parÃ©s
```bash
# Backend avec Gunicorn
gunicorn -w 4 -k uvicorn.workers.UvicornWorker backend.main:app

# Frontend avec Nginx reverse proxy
streamlit run frontend/app.py --server.port=8501
```

## ğŸ“ˆ Performances

- âš¡ Parse 10K logs/seconde
- ğŸ’¾ Base SQLite jusqu'Ã  1M de logs
- ğŸš€ API response < 100ms
- ğŸ§  ML training < 5 secondes

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Ouvrez une issue ou PR.

## ğŸ“ Support

CrÃ©Ã© dans le cadre du challenge **50 projets Python en 50 jours**
Projet 9/50 âœ…

---

**â­ N'oubliez pas de mettre une Ã©toile si ce projet vous a Ã©tÃ© utile !**

